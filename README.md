# Engram MCP

<p align="center">
  <strong>A local, sovereign, hybrid memory system for MCP-compatible agents</strong>
</p>

<p align="center">
  Semantic search ‚Ä¢ Lexical search ‚Ä¢ Persistent memory ‚Ä¢ Non-blocking ‚Ä¢ Secure by default
</p>

---

## üß† What is Engram MCP?

**Engram MCP** is a high‚Äëperformance, open‚Äësource memory server that gives AI agents *long‚Äëterm, searchable memory*.

In neuroscience, an **engram** is the physical trace of a memory in the brain.  
Engram MCP brings that concept to AI systems: durable, auditable, local memory that scales from small projects to universal personal knowledge bases.

Engram is **not** a chat log.  
It is **not** a volatile cache.  

It is a **cognitive substrate** for agents.

---

## ‚ú® Core Features

### üîç Hybrid Search (State of the Art)
- **Vector similarity search** (FAISS)
- **Lexical BM25 search** (SQLite FTS5, on‚Äëdisk)
- **Reciprocal Rank Fusion (RRF)** for best‚Äëof‚Äëboth‚Äëworlds ranking

### ‚ö° Non‚ÄëBlocking Architecture
- Embeddings offloaded from the event loop
- Async SQLite via `aiosqlite`
- Long indexing jobs do **not** freeze search or control tools

### üóÇÔ∏è Persistent & Incremental Indexing
- Chunk‚Äëlevel hashing & deduplication
- `ON CONFLICT` upserts
- Automatic re‚Äëindexing when files change

### üîê Security‚ÄëFirst by Default
- Explicit **path whitelisting**
- Protection against path traversal & symlink escape
- Refuses to index outside configured roots

### üß© MCP‚ÄëNative
- Built on the **official MCP SDK / FastMCP**
- No manual JSON‚ÄëRPC parsing
- Forward‚Äëcompatible with protocol changes

### üß™ Production‚ÄëGrade
- Modular architecture
- Clear separation of concerns
- Suitable for long‚Äërunning agent workloads

---

## üèóÔ∏è Architecture Overview

```
engram_mcp/
‚îú‚îÄ‚îÄ server.py            # MCP entrypoint
‚îú‚îÄ‚îÄ config.py            # Configuration & security guards
‚îú‚îÄ‚îÄ indexing/
‚îÇ   ‚îú‚îÄ‚îÄ indexer.py       # File discovery & chunking
‚îÇ   ‚îî‚îÄ‚îÄ workers.py       # Background execution
‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îî‚îÄ‚îÄ encoder.py       # Embedding model logic
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql       # Tables, FTS5, triggers
‚îÇ   ‚îî‚îÄ‚îÄ store.py         # Async database access
‚îú‚îÄ‚îÄ search/
‚îÇ   ‚îî‚îÄ‚îÄ hybrid.py        # Vector + BM25 + RRF
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ paths.py
‚îÇ   ‚îî‚îÄ‚îÄ hashing.py
‚îî‚îÄ‚îÄ engram_mcp.yaml.example
```

---

## üöÄ Quick Start

### 1Ô∏è‚É£ Install Engram MCP (pipx or uv)

**pipx (recommended for CLI usage)**
```bash
pipx install engram-mcp
engram-mcp
```

Enable vector search with FAISS CPU (optional):
```bash
pipx install "engram-mcp[cpu]"
```

Enable FAISS GPU builds (optional, Linux-first support):
```bash
pipx install "engram-mcp[gpu]"
```

**uv (fast, reproducible dev install)**
```bash
uv venv
uv pip install -e ".[cpu]"
uv run engram-mcp
```

> Base installs are **FTS-only** (no FAISS, no Numba JIT). CPU/GPU extras enable vector search.

---

### 2Ô∏è‚É£ Configure allowed paths

Engram reads its config from a **user-scoped location by default**:

| OS | Config path |
| --- | --- |
| Linux | `~/.config/engram/engram_mcp.yaml` |
| macOS | `~/.config/engram/engram_mcp.yaml` |
| Windows | `%APPDATA%\\engram\\engram_mcp.yaml` |

Create the file there (or set `ENGRAM_CONFIG_PATH` to a file *inside* that directory).

```bash
mkdir -p ~/.config/engram
cp engram_mcp.yaml.example ~/.config/engram/engram_mcp.yaml
```

Edit **`allowed_roots`**:

```yaml
allowed_roots:
  - /Users/you/Documents
  - /Users/you/Projects
```

> ‚ö†Ô∏è Engram **will refuse** to index anything outside these paths.

---

### 3Ô∏è‚É£ Run the server
```bash
engram-mcp
```

SentenceTransformers models are downloaded on first use unless you pre-download them (for air‚Äëgapped setups, cache the models or point `model_name_*` to a local path).  
No surprises once the model cache is in place.

Engram MCP is now available to MCP‚Äëcompatible clients.

---

## ‚öôÔ∏è Runtime Modes & Dependencies

Engram MCP starts in **FTS-only** mode by default (no FAISS, no Numba). Vector search is enabled when FAISS is installed.

Optional config flags in `engram_mcp.yaml`:
```yaml
vector_backend: auto   # auto | fts | faiss_cpu | faiss_gpu
enable_numba: false    # opt-in JIT kernels
search_cache_ttl_s: 300
search_cache_max_items: 512
```

On startup, Engram logs:
- storage paths (DB/index)
- vector search mode (and how to enable)
- numba status
- search cache status

---

## üß† MCP Tools

### `index_project`
Indexes a directory (must be within `allowed_roots`).

```json
{
  "directory": "/Users/you/Projects/my_repo",
  "project_name": "my_repo",
  "project_type": "code",
  "wait": true
}
```

---

### `search_memory`
Hybrid semantic + lexical search.

```json
{
  "query": "async sqlite performance issues",
  "project_id": "my_repo_123",
  "max_results": 10,
  "fts_mode": "strict"
}
```

---

### `update_project`
Updates an existing project (use `wait: false` to queue and poll job status).

```json
{
  "project_id": "my_repo_123",
  "wait": true
}
```

### `delete_project`
Removes all indexed content for a project root.

---

## üîí Security Model

Engram MCP **will refuse** to index paths that:

- Are outside `allowed_roots`
- Escape via symlinks
- Attempt traversal (`../`)

This prevents accidental indexing of:
- `/`
- `.ssh`
- System files
- Private or sensitive directories

Security is **opt‚Äëin by configuration**, not implicit trust.

### Storage defaults (privacy-safe)
By default, Engram writes all state into a **user-scoped data directory** (never the current working directory):

| OS | Data directory |
| --- | --- |
| Linux | `~/.local/share/engram/` |
| macOS | `~/Library/Application Support/engram/` |
| Windows | `%APPDATA%\\engram\\` |

The SQLite DB and FAISS index files are created with **owner-only permissions**. Override `db_path`/`index_dir`
explicitly if you want storage in a custom location. On Windows, permissions are best-effort and may rely
on the existing directory ACLs.

### Query limits
`search_memory` enforces `max_query_chars` and `max_query_tokens` from config (defaults: 4096 chars / 256 tokens)
to prevent runaway embedding costs.

---

## ‚öôÔ∏è Performance Notes

### Embeddings
- Executed off the event loop
- CPU ‚Üí `ProcessPoolExecutor`
- CUDA ‚Üí thread‚Äësafe execution (no fork hazards)

### Search
- Vector search via FAISS
- Lexical search via SQLite FTS5
- No in‚Äëmemory BM25 structures

### Search mode
`search_memory` supports `fts_mode`:
- `strict` (default): `AND` across tokens
- `any`: `OR` across tokens

Quoted phrases are preserved in either mode.

## üß™ Development

### Dependency locking
This repo uses **pip-tools**. Update locks with:

```bash
pip install pip-tools
pip-compile requirements.in
pip-compile requirements-dev.in
```

### Tests
```bash
pytest
```

### Indexing
- Streaming file readers
- No `f.read()` on large files
- Safe for multi‚ÄëGB corpora

---

## üß† Philosophy

Large Language Models are powerful ‚Äî but **stateless**.

Engram MCP exists to give agents:
- Memory that persists
- Knowledge they can revisit
- Context they can build upon

This is not convenience infrastructure.

This is **cognition infrastructure**.

---

## üõ£Ô∏è Roadmap

- Binary vector quantization (32√ó memory reduction)
- Tantivy / Lucene backend option
- Namespace isolation & multi‚Äëtenant memory
- Hot‚Äëswappable embedding models
- Cross‚Äëagent shared memory graphs

---

## ü§ù Contributing

Contributions are welcome.

If you are interested in:
- Retrieval systems
- Agent architectures
- Local‚Äëfirst AI
- High‚Äëperformance Python

You‚Äôll feel at home here.

---

## üìú License

MIT License.

Build agents that remember.
